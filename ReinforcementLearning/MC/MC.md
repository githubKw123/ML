## 强化学习基本框架

**两个主体：** Agent Environment
**一个框架：** MDP
**五大元素：** $S,A,R$(三集合) $\pi,p(s',r|s,a)$(两分布)

**核心问题：** 如何找到最优策略？

1. 价值函数 $V_{\pi}, q_{\pi}$
2. 策略迭代：策略评估，策略改进
3. 价值迭代

> 这些方法的一个前提是，动态特性是已知的（Model-Based），那么面对动态特性未知（Model-Free）的情况下该如何处理，这里就需要用到蒙特卡洛的方法。

## MC策略评估

考虑到贝尔曼方程中$p(s',r|s,a)$这一部分得不到，我们可以考虑到直接用前面期望这一部分

$$
V_{\pi}(s) = \mathbb E_{\pi}[G_t|S_t=s]=\sum_{a\in \mathcal A} \pi(a|s) \sum_{r,s'} p(s',r|s,a) [r + \gamma V_{\pi}(s')]
$$

也就是直接借助大数定理，多次收集轨迹，计算每个状态的平均回报以此代替

$$
V_{\pi}(s) = \frac1{N} \sum_{i=1}^NG_t
$$

步骤：

1. 每个Episod，采样一组轨迹$S_0,A_0,R_0,...,S_T,...$
2. 从后向前遍历，计算这个轨迹中每个状态的$G$:$G=\gamma G_{t+1}+R_{t+1}$
3. 若$S_t$未出现过，将其对应的$G_t$加入到$Return（S_t）$（类似于一个Hashmap）中
4. $V(S_t) \leftarrow average(Return(S_t))$
5. 下一个Episod

注意：上边这是针对$V_{\pi}$的处理方式。其实$q_{\pi}$也是同样的，但对于$q_{\pi}$有两个问题：

一是$q_{\pi}$需要采样$(s,a)$对的数据，因此需要更多的数据
二是试探性出发假设：也就是每种$(s,a)$对都应该以非零的概率作为起点（这是为了确保每个状态动作对都有采样到的可能）

## MC控制

类似于DP算法还是两步：

1. 策略评估：MC方法评估$q_{\pi}$(条件：1.无线幕 2.试探性出发假设)
2. 策略改进：$\pi'(s) = \argmax_aq_{\pi}(s,a)$

针对策略改进的两个条件，解决方法：

对于无线幕的问题，用GPI的方法，只取一幕进行处理

对于试探性出发假设，有两种方法：

同轨策略方法（On-policy）：行动策略等于目标策略,都必须为软策略，也就是随机性策略且每种$(s,a)$被选择到概率都有可能，这样的话就有些苛刻

离轨策略方法（Off-policy）：行动策略不等于目标策略，这样的话行动策略必须为软性的，目标策略无所谓，没有必然的联系，然而对于离轨的方法就不能单纯的用普通的采样方式，因为这样行动策略采样的数据是不能用于更新目标策略的$q_{\pi}$的，因此这里要使用重要性采样

#### 同轨策略方法

同轨策略方法的关键是怎么选择一种软性策略，这里使用了一种贪心策略，也就是仍然将$q_{\pi}$最大的动作设置为概率最大，其他动作概率平均

$\epsilon$-贪心策略：给定$\pi$,$q_{\pi}(s,a)$,如果动作价值最大$\pi'(a|s)=1-\epsilon+\frac{\epsilon}{|A(s)|}$,其他的为$\frac{\epsilon}{|A(s)|}$

证明这种贪心策略优于原策略的：

#### 异轨策略方法

给定 目标策略$\pi$ 和 行动策略$b$

1. **策略评估** ：给定$\pi$，怎么求$V_{\pi}$

$$
V_{\pi}(s) = \mathbb E_{\pi}[G_t|S_t=s] = \sum G_t\pi(A_t|S_t)p(R_{t+1},S_{t+1}|S_t,A_t)...\pi(A_t|S_t)p(R_T,S_T)
$$

这一步相当于把马尔可夫展开，也就是从每个状态开始的往后的马氏链乘以对应的回报

$$
=\sum G_t \prod_{k=t}^{T-1} [\pi(A_k|S_k)p(R_{k+1},S_{k+1}|S_k,A_k)] \\
=\sum \prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k)} G_t [b(A_k|S_k) p(R_{k+1},S_{k+1}|S_k,A_k)]
$$

这里就可以把行动策略$b$加进来，前面的分式可以求出来，可以看作一个重要性的权重，进一步的结合MC评估进行

$$
V_{\pi}(s)=\sum \prod_{k=t}^{T-1}\rho_{t:T-1} G_t [b(A_k|S_k) p(R_{k+1},S_{k+1}|S_k,A_k)] \\
=\mathbb E_{b}[\rho_{t:T-1} G_t|S_t=s] \\
=\frac1{\sum_{i=1}^N \rho_{t:T-1}} \sum_{i=1}^N \rho_{t:T-1} G_t
$$

增量更新：不同于传统的MC策略评估方法，这种重要性采样在第四部更新$V_{\pi}$时，不再是$V(S_t) \leftarrow average(Return(S_t))$，而是要加入权重的部分，变为：

$$
权重：c \leftarrow c + w \\
状态价值： V \leftarrow V +\frac{w}{c}(G-V)
$$

2. **策略迭代**：跟之前的没啥区别，在策略评估时可以直接评估$q$，然后贪心$\pi'(s) = \argmax_aq_{\pi}(s,a)$就可以了

