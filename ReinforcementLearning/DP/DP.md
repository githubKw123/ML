# 动态规划

这一节主要是用**动态规划方法**求解MDP问题，包括策略迭代和价值迭代两种方式

### 策略迭代

策略迭代包括以下两个过程：

1. 策略评估：给定策略，确定策略的价值函数
2. 策略迭代：迭代策略，让新策略优于原策略的价值函数

##### 策略评估：已知MDP动态过程$p(s',r|s,a)$,给定$\pi$,求$V_{\pi}$,记$V_{\pi}=(V_{\pi}(s_1),V_{\pi}(s_2),...,V_{\pi}(s_{|s|}))^T$.

依照贝尔曼方程：

$$
V_{\pi}(s) = \sum_{a\in \mathcal A} \pi(a|s)Q_{\pi}(s,a)= \sum_{a\in \mathcal A} \pi(s|a) \sum_{r,s'} p(s',r|s,a) [r + \gamma V_{\pi}(s')]
$$

这里把$V_{\pi}(s)$看作一个未知数的话这本质上就是一个线性方程,那我们把$V_{\pi}$矩阵的形式带进去，对公式进行化简，就可以求出一个解析解（考虑到解析解的求法，一是比较复杂，二是复杂度特别高，一般不用），这里重点介绍迭代策略评估的方法：

主要思想：构造一个$\{V_{\pi}\}^{\infty}_{k=1}$,让其收敛到$V_{\pi}$
迭代公式：$V_{k+1}(s) = \sum_{a\in \mathcal A} \pi(a|s) \sum_{r,s'} p(s',r|s,a) [r + \gamma V_{k}(s')]$

##### 策略改进：给定$\pi$,计算得到$\pi'$，让其优于$\pi$

那么怎么判断$\pi'$要优于$\pi$呢，这里可以借助策略改进定理：

* 给定$\pi$,$\pi'$,如果$\forall s \in S$, $q_{\pi}(s,\pi'(s)) \ge V_{\pi}(s)$, 那么有$\forall s \in S$，$V_{\pi}'(s) \ge V_{\pi}(s)$

那么有了判断方法，怎么取$\pi'$,这里介绍贪心策略:

* 对于$\forall s \in S$, 取 $\pi'(s) = \argmax_aq_{\pi}(s,a)$

> 简单证明：$\forall s \in S$, $V_{\pi}(s) \le \max_a q_{\pi}(s,a) = q_{\pi}(s,\pi'(s))$, 根据上面的定理，得证。
> 注意：当$V_{\pi}'(s) = V_{\pi}(s)$，就说明贪心策略找不到更好的了，此时达到了$V_*$

那么算法整体的流程就是：

1. 根据给定的$\pi$，用策略评估计算$V_{\pi}$和$q_{\pi}$.
2. 贪心算法得到新的$\pi'(s) = \argmax_aq_{\pi}(s,a)$.
3. 重复直至$V_{\pi}'(s) = V_{\pi}(s)$

### 价值迭代

这里有一个问题：策略评估部分也是要迭代的，这就产生了一个迭代套迭代的问题，这里考虑到中间部分的各个$V_{\pi}(s)$是不这么重要的，所以这里策略评估的时候可以直接截断，就原先假如需要进行100次，这里可以只进行1次。

那么这样就可以结合贪心策略

$a_{new} = argmax_aq_{k+1}(s,a)$

对应的 $V_{k+1}(s) =max_a q_{k+1}(s,a)$

加上策略评估走了一步 $q_{k+1}(s,a) = \sum_{r,s'} p(s',r|s,a) [r + \gamma V_{k}(s')]$

得到了$V_{k+1}(s) = max_a \sum_{r,s'} p(s',r|s,a) [r + \gamma V_{k}(s')]$

可以发现这个跟策略完全没有关系了，只需要不断迭代$V_{\pi}$就行，这就是价值迭代。

还有没有更简单的：就算走一步策略评估迭代也要计算所有的状态，我们再简化一下，只选择几个状态迭代，就得到了异步策略评估迭代




